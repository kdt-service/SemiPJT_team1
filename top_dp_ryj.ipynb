{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 탑리뷰어 df\n",
    "\n",
    "작성자, 주문 날짜, 리뷰 작성 날짜, 리뷰 내용, 리뷰당 ‘도움이 돼요’ 개수, 한달 이상 사용, 재구매 여부, 사진 개수, 제품 구매 방법, 리뷰 고유번호, 평점(긍부정)\n",
    "리뷰 쓴 주기, 리뷰 길이, 총 ‘도움이 돼요’, 리뷰당 ‘도움이 돼요’ (평균), 체험단 리뷰와 일반 리뷰 차이 (리뷰 길이 편차)\n",
    "\n",
    "- gdasSeq  :  리뷰 고유번호\n",
    "- gdasScrVal: 평점 (2,4,6,8,10)\n",
    "- dispRegDate : 리뷰 업로드 날짜\n",
    "- gdasCont : 리뷰 본문\n",
    "- ordDate: 상품 주문 날짜\n",
    "- photoList: 사진 리스트 (사진 없으면 null)\n",
    "- recommCnt: 도움이 돼요 개수\n",
    "- renewUsed1mmGdasYn: 한달이상 사용\n",
    "- firstGdasYn: 첫 구매 여부 (재구매면 “N” 아니면 “Y”)\n",
    "- mbrNo: 리뷰를 작성한 회원의 고유 식별자\n",
    "- gdasSctCd : 제품 구매 방법 (10 - 온라인, 60 - 매장, 50 - 체험단, 70 - 선물)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/w592whfs2tb4g8s658288fb00000gn/T/ipykernel_50605/3744978623.py:3: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  top_df = pd.read_csv(topReviews_path, header=0, encoding='UTF-8')\n"
     ]
    }
   ],
   "source": [
    "# 탑리뷰어 정보 DF로 가져오기\n",
    "topReviews_path = '/Users/ryeongjoo/Desktop/workspace/semi_prj/topReviews.csv'\n",
    "top_df = pd.read_csv(topReviews_path, header=0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 칼럼만 담은 DF\n",
    "my_top_df = top_df[['mbrNo','ordDate','dispRegDate','gdasCont','recommCnt','renewUsed1mmGdasYn','firstGdasYn', 'gdasSeq', 'gdasScrVal','gdasSctCd' ]]\n",
    "# print(my_top_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN 값 드랍\n",
    "my_top_df = my_top_df.dropna(subset=['gdasCont'])\n",
    "\n",
    "# 영어 소문자 통일\n",
    "my_top_df['gdasCont'] = my_top_df['gdasCont'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식을 이용하여 이모티콘 추출하는 함수\n",
    "def extract_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001f600-\\U0001f64f\"  # 이모티콘\n",
    "        u\"\\U0001f300-\\U0001f5ff\"  # 기호 및 문장 부호\n",
    "        u\"\\U0001f680-\\U0001f6ff\"  # 기타 이모티콘\n",
    "        u\"\\U0001f1e0-\\U0001f1ff\"  # 국기 및 기호\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.findall(text)\n",
    "\n",
    "# '이모티콘' 열 생성\n",
    "my_top_df['unicode'] = my_top_df['gdasCont'].apply(lambda x: extract_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰내용 중복문자, 이모티콘 제거\n",
    "clean_reviews = []\n",
    "emoticon_list = []\n",
    "\n",
    "def remove_emoji(text):\n",
    "    # 이모티콘 패턴 정의\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # 이모티콘\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # 심볼\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # 트랜스포트 및 지도\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # 국기 및 이모지\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    # 이모티콘 제거\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "for review in my_top_df['gdasCont'] :\n",
    "    \n",
    "    # 초성, 줄바꿈 제거\n",
    "    review = re.sub('[ㄱ-ㅎㅏ-ㅣ]', '', review)\n",
    "    review = re.sub('[\\r\\n]', ' ', review)  \n",
    "    \n",
    "    # 이모티콘 제거\n",
    "    review = remove_emoji(review)\n",
    "    \n",
    "    clean_reviews.append(review)\n",
    "    \n",
    "my_top_df['gdasCont'] = clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 길이 칼럼 추가\n",
    "my_top_df['cont_length'] = my_top_df['gdasCont'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# print(my_top_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 주기 칼럼\n",
    "# 리뷰 작성 날짜 - 구매일자\n",
    "\n",
    "# 날짜형으로 변환\n",
    "my_top_df['dispRegDate'] = pd.to_datetime(my_top_df['dispRegDate'])\n",
    "my_top_df['ordDate'] = pd.to_datetime(my_top_df['ordDate'])\n",
    "\n",
    "# NaN이 아닌 행 선택\n",
    "subset = my_top_df[my_top_df['ordDate'].notnull() ]\n",
    "\n",
    "# 두 날짜의 차이를 계산하여 새로운 칼럼에 추가\n",
    "my_top_df.loc[subset.index, 'or_diff'] = my_top_df['dispRegDate'] - my_top_df['ordDate']\n",
    "\n",
    "# 결과 출력\n",
    "# print(my_top_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리뷰어별 df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mbrNo 칼럼 생성\n",
    "unique_mbrNo = my_top_df['mbrNo'].unique()  # 중복을 제거한 mbrNo 값 추출\n",
    "reviewer_df['mbrNo'] = pd.Series(unique_mbrNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰어별 총 도움이 돼요 개수\n",
    "\n",
    "# 총 리뷰 개수 계산\n",
    "recommSum_df = my_top_df.groupby('mbrNo')['recommCnt'].sum().reset_index().rename(columns={'recommCnt': 'total_recomm'})\n",
    "\n",
    "# reviewer_df와 recommSum_df 조인\n",
    "reviewer_df = pd.merge(reviewer_df, recommSum_df, on='mbrNo', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰어별 작성 리뷰 개수\n",
    "reviewCnt_df = my_top_df.groupby('mbrNo')['gdasCont'].count().reset_index().rename(columns={'gdasCont': 'gdas_cnt'})\n",
    "reviewer_df = pd.merge(reviewer_df, reviewCnt_df, on='mbrNo', how='left')\n",
    "\n",
    "# 리뷰어별 리뷰 개수당 평균 도움이 돼요 개수\n",
    "reviewer_df['recomm_mean'] = reviewer_df['total_recomm'] / reviewer_df['gdas_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰어별 체험단 리뷰와 일반 리뷰 차이 (리뷰 길이 편차)\n",
    "\n",
    "# 체험단 리뷰길이 평균\n",
    "test_df = my_top_df.loc[my_top_df['gdasSctCd'] == 50].groupby('mbrNo')['cont_length'].mean()\n",
    "\n",
    "# 체험단 제외한 리뷰 길이 평균\n",
    "buy_df = my_top_df.loc[my_top_df['gdasSctCd'] != 50].groupby('mbrNo')['cont_length'].mean()\n",
    "\n",
    "# 체험단 리뷰길이 - 체험단 제외한 리뷰 길이 \n",
    "diff_df = pd.DataFrame({'tb_len_diff': test_df - buy_df}).reset_index()\n",
    "\n",
    "# reviewer_df와 recommSum_df 조인\n",
    "reviewer_df = pd.merge(reviewer_df, diff_df, on='mbrNo', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mbrNo</th>\n",
       "      <th>total_recomm</th>\n",
       "      <th>gdas_cnt</th>\n",
       "      <th>recomm_mean</th>\n",
       "      <th>tb_len_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M0000004535694</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>300</td>\n",
       "      <td>3.393333</td>\n",
       "      <td>222.132203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M0000012163234</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>80</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>310.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M0000016082816</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>110</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>222.391509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M0000003453847</td>\n",
       "      <td>604.0</td>\n",
       "      <td>100</td>\n",
       "      <td>6.040000</td>\n",
       "      <td>249.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M0000010425691</td>\n",
       "      <td>765.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>36.406250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mbrNo  total_recomm  gdas_cnt  recomm_mean  tb_len_diff\n",
       "0  M0000004535694        1018.0       300     3.393333   222.132203\n",
       "1  M0000012163234        1216.0        80    15.200000   310.120000\n",
       "2  M0000016082816        1177.0       110    10.700000   222.391509\n",
       "3  M0000003453847         604.0       100     6.040000   249.791667\n",
       "4  M0000010425691         765.0       100     7.650000    36.406250"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    제가 최근에 올리브영에서 가장 많이구매를 했던 제품중 하나입니다신랑이 30g짜리를 ...\n",
       "1    허브쏠트 믹스넛은 견과류구성은맘에 드는데 넘짜요ㅠ그래도 1+1 행사를 하길래 구매를...\n",
       "2    아침에 주로 마시고 있는 사과당근맛이너주스입니다제가 5년을 넘게 마시고 있는데 꾸준...\n",
       "3    신랑을 줄려고 신청을 했어요다비도프 향수는 프랑스에서 만들어졌고 향수병은 네모난 형...\n",
       "4              팽이버섯 유산균 발효액이 들어가서꾸준하게 마시면 화장실 가는게편해지네요\n",
       "5    최근에 올리브영에서 제일 많이구매를 했던 제품입니다신랑이 거의 매일 30g짜리를 두...\n",
       "6    입자가 정말 고와요펄감도 적당하고 정말 괜찮은듯 합니다무엇보다 가루날림이 없어서 맘...\n",
       "7    아침에 주로 마시고 있는 사과당근맛이너주스입니다이너주스는 프룬맛과 사과당근맛 두가지...\n",
       "8            이너주스중 제가 제일 좋아하는프룬맛입니다120ml라서 좀 아쉬울때도 있어요\n",
       "9    신랑이 거의매일 30g짜리두봉지씩 먹고있어요그래서 올리브영에서 1+1행사를할때 계속...\n",
       "Name: gdasCont, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def text_len(text):\n",
    "    return re.sub('[^ㅏ-ㅣ가-힣 .,+…\\';:\\-ㆍ\\(\\)&0-9A-Za-z]+', '', text)\n",
    "my_top_df['gdasCont'][:10].apply(text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mbrNo  total_recomm  gdas_cnt  recomm_mean  tb_len_diff\n",
      "0  M0000004535694        1018.0       300     3.393333   222.132203\n",
      "1  M0000012163234        1216.0        80    15.200000   310.120000\n",
      "2  M0000016082816        1177.0       110    10.700000   222.391509\n",
      "3  M0000003453847         604.0       100     6.040000   249.791667\n",
      "4  M0000010425691         765.0       100     7.650000    36.406250\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리뷰 내용 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['제', '최근', '올리브영', '가장', '많이', '구매', '했던', '제품', '중', '하나', '입니다', '신랑', '30', 'g', '짜', '리', '하루', '두', '봉지', '씩', '먹네', '요하', '제입', '많이', '달아요'], ['허브', '쏠트', '믹스넛', '견과류', '구성은', '맘', '드는데', '넘', '짜요', '그래도', '1', '1', '행사', '하길래', '구매', '했는데', '겉', '묻은', '가루', '털어내고', '먹어야', '해'], ['아침', '주로', '마시고', '있는', '사과', '당', '근', '맛', '이', '너', '주스', '입니다', '제', '5년', '을', '넘게', '마시고', '있는데', '꾸준하게', '마시면', '화장실', '가는게', '편해져서', '좋아요'], ['신랑', '줄려고', '신청', '했어요', '다비도프', '향수', '프랑스', '만들어졌고', '향수병', '네모', '형태', '되어있고', '잡기', '편하네요', '제품', '받고', '바로', '신랑', '뿌려', '봤는데', '향', '딱', '남자', '들', '좋아할만', '한', '향', '요', '일단', '향', '중요하지만', '지속', '력', '좋아야', '해서', '신랑', '뿌리', '한두', '시간', '지나서도', '향', '나는지', '옆', '가봤더니', '진하게는아니지만', '은은하게', '잔향', '남아', '있는듯', '하네요', '😁', '날씨', '점점', '더워지는데', '여름', '뿌려도', '좋을것', '같다는', '생각', '드', '네', '나이', '상관없이', '모든', '연령', '층', '사용', '하기에', '괜찮은', '제품', '듯', '합니다', '👍'], ['팽이버섯', '유산균', '발효', '액', '들어가서', '꾸준하게', '마시면', '화장실', '가는게', '편해지네요']]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "tokens_results = []\n",
    "\n",
    "for text in my_top_df['gdasCont'].head(5):\n",
    "    \n",
    "    # Okt 객체 생성\n",
    "    okt = Okt()\n",
    "\n",
    "    # 초성, 줄바꿈 제거\n",
    "    text = re.sub('[ㄱ-ㅎㅏ-ㅣ\\r\\n]', '', text)\n",
    "\n",
    "    tokens = okt.pos(text)\n",
    "    result = []\n",
    "\n",
    "    for word, tag in tokens:\n",
    "        if tag not in ['Josa', 'Eomi', 'Punctuation']:\n",
    "            result.append(word)\n",
    "\n",
    "    tokens_results.append(result)\n",
    "    \n",
    "print(tokens_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30' '가장' '구매' '달아요' '많이' '먹네' '봉지' '신랑' '올리브영' '요하' '입니다' '제입' '제품' '최근'\n",
      " '하나' '하루' '했던']\n",
      "['가루' '견과류' '구매' '구성은' '그래도' '드는데' '먹어야' '묻은' '믹스넛' '쏠트' '짜요' '털어내고' '하길래'\n",
      " '했는데' '행사' '허브']\n",
      "['5년' '가는게' '꾸준하게' '넘게' '마시고' '마시면' '사과' '아침' '입니다' '있는' '있는데' '좋아요' '주로'\n",
      " '주스' '편해져서' '화장실']\n",
      "['가봤더니' '같다는' '괜찮은' '나는지' '나이' '날씨' '남아' '남자' '네모' '다비도프' '더워지는데' '되어있고'\n",
      " '만들어졌고' '모든' '바로' '받고' '봤는데' '뿌려' '뿌려도' '뿌리' '사용' '상관없이' '생각' '시간' '신랑'\n",
      " '신청' '여름' '연령' '은은하게' '일단' '있는듯' '잔향' '잡기' '점점' '제품' '좋아야' '좋아할만' '좋을것'\n",
      " '줄려고' '중요하지만' '지나서도' '지속' '진하게는아니지만' '편하네요' '프랑스' '하기에' '하네요' '한두' '합니다'\n",
      " '해서' '했어요' '향수' '향수병' '형태']\n",
      "['가는게' '꾸준하게' '들어가서' '마시면' '발효' '유산균' '팽이버섯' '편해지네요' '화장실']\n"
     ]
    }
   ],
   "source": [
    "# 키워드 추출\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for documents in tokens_results:\n",
    "\n",
    "    # TfidfVectorizer 객체 생성\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # TfidfVectorizer로 문서 벡터화 수행\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "    # get_feature_names() 메서드를 이용해 단어 목록과 각 단어의 인덱스 확인\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하지만 제입엔 많이 달아요 신랑이 30g짜리를 하루에 두봉지씩\n",
      "먹네요ㅋ 제가 최근에 올리브영에서 가장 많이\n",
      "구매를 했던 제품중 하나입니다ㅎ\n",
      "그래도 1+1 행사를 하길래 구매를\n",
      "했는데 겉에 묻은 가루를 털어내고\n",
      "먹어야 해요 허브쏠트 믹스넛은 견과류구성은\n",
      "맘에 드는데 넘짜요ㅠ\n",
      "제가 5년을 넘게 마시고 있는데 꾸준하게\n",
      "마시면 화장실 가는게 편해져서 좋아요 아침에 주로 마시고 있는 사과당근맛\n",
      "이너주스입니다ㅎ\n",
      "신랑을 줄려고 신청을 했어요~ 일단 향도 중요하지만 지속력도 좋아야\n",
      "해서 신랑이 뿌리고 한두시간 지나서도\n",
      "향이 나는지 옆에 가봤더니 진하게는\n",
      "아니지만 은은하게 잔향이 남아 있는듯\n",
      "하네요😁 나이와 상관없이 모든 연령층이 사용하기에\n",
      "괜찮은 제품인듯 합니다👍\n",
      "팽이버섯 유산균 발효액이 들어가서\n",
      "꾸준하게 마시면 화장실 가는게\n",
      "편해지네요ㅎㅎ\n"
     ]
    }
   ],
   "source": [
    "# 요약\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import kss\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "for document in my_top_df['gdasCont'].head(5):\n",
    "    # Okt 객체 생성\n",
    "    okt = Okt()\n",
    "\n",
    "    # 문장 단위로 분리\n",
    "    sentences = kss.split_sentences(document)\n",
    "\n",
    "    # 각 문장에서 명사, 형용사, 동사만 추출하여 리스트에 저장\n",
    "    words_list = []\n",
    "    for sentence in sentences:\n",
    "        words = okt.pos(sentence, stem=True)\n",
    "        words = [word[0] for word in words if word[1] in ['Noun', 'Adjective', 'Verb']]\n",
    "        words_list.append(' '.join(words))\n",
    "\n",
    "    # TF-IDF를 계산하여 문장 간 유사도를 계산\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(words_list)\n",
    "    similarity_matrix = cosine_similarity(tfidf, tfidf)\n",
    "\n",
    "    # TextRank 알고리즘을 적용하여 문서 요약\n",
    "    def summarize(document, num_sentences=3):\n",
    "\n",
    "\n",
    "        # 각 문장에서 명사, 형용사, 동사만 추출하여 리스트에 저장\n",
    "        words_list = []\n",
    "        for sentence in sentences:\n",
    "            words = okt.pos(sentence, stem=True)\n",
    "            words = [word[0] for word in words if word[1] in ['Noun', 'Adjective', 'Verb']]\n",
    "            words_list.append(' '.join(words))\n",
    "\n",
    "        # TF-IDF를 계산하여 문장 간 유사도를 계산\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf = vectorizer.fit_transform(words_list)\n",
    "        similarity_matrix = cosine_similarity(tfidf, tfidf)\n",
    "\n",
    "        # TextRank 알고리즘을 적용하여 문서 요약\n",
    "        similarity_graph = similarity_matrix\n",
    "        nx_graph = nx.from_numpy_array(similarity_graph)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "\n",
    "        # 핵심 문장 추출\n",
    "        ranked_sentences = sorted(((score, index) for index, score in scores.items()), reverse=True)\n",
    "        selected_sentences = [sentences[index] for _, index in ranked_sentences[:num_sentences]]\n",
    "        return ' '.join(selected_sentences)\n",
    "\n",
    "    # 문서 요약\n",
    "    summary = summarize(document, num_sentences=3)\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하지만 제입엔 많이 달아요 신랑이 30g짜리를 하루에 두봉지씩  먹네요 제가 최근에 올리브영에서 가장 많이  구매를 했던 제품중 하나입니다\n",
      "그래도 1+1 행사를 하길래 구매를  했는데 겉에 묻은 가루를 털어내고  먹어야 해요 허브쏠트 믹스넛은 견과류구성은  맘에 드는데 넘짜요\n",
      "제가 5년을 넘게 마시고 있는데 꾸준하게  마시면 화장실 가는게 편해져서 좋아요 아침에 주로 마시고 있는 사과당근맛  이너주스입니다\n",
      "신랑을 줄려고 신청을 했어요~ 일단 향도 중요하지만 지속력도 좋아야  해서 신랑이 뿌리고 한두시간 지나서도  향이 나는지 옆에 가봤더니 진하게는  아니지만 은은하게 잔향이 남아 있는듯  하네요😁 나이와 상관없이 모든 연령층이 사용하기에  괜찮은 제품인듯 합니다👍\n",
      "팽이버섯 유산균 발효액이 들어가서  꾸준하게 마시면 화장실 가는게  편해지네요\n"
     ]
    }
   ],
   "source": [
    "# 요약2\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import kss\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "for document in my_top_df['gdasCont'].head(5):\n",
    "    # Okt 객체 생성\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 초성, 줄바꿈 제거\n",
    "    document = re.sub('[ㄱ-ㅎㅏ-ㅣ]', '', document)\n",
    "    document = re.sub('[\\r\\n]', ' ', document)\n",
    "    \n",
    "    # 문장 단위로 분리\n",
    "    sentences = kss.split_sentences(document)\n",
    "    \n",
    "    # 각 문장에서 명사, 형용사, 동사만 추출하여 리스트에 저장\n",
    "    words_list = []\n",
    "    for sentence in sentences:\n",
    "        words = okt.pos(sentence, stem=True)\n",
    "        words = [word[0] for word in words if word[1] in ['Noun', 'Adjective', 'Verb']]\n",
    "        words_list.append(' '.join(words))\n",
    "\n",
    "    # TF-IDF를 계산하여 문장 간 유사도를 계산\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform(words_list)\n",
    "    similarity_matrix = cosine_similarity(tfidf, tfidf)\n",
    "\n",
    "    # TextRank 알고리즘을 적용하여 문서 요약\n",
    "    def summarize(document, num_sentences=3):\n",
    "\n",
    "\n",
    "        # 각 문장에서 명사, 형용사, 동사만 추출하여 리스트에 저장\n",
    "        words_list = []\n",
    "        for sentence in sentences:\n",
    "            words = okt.pos(sentence, stem=True)\n",
    "            words = [word[0] for word in words if word[1] in ['Noun', 'Adjective', 'Verb']]\n",
    "            words_list.append(' '.join(words))\n",
    "\n",
    "        # TF-IDF를 계산하여 문장 간 유사도를 계산\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf = vectorizer.fit_transform(words_list)\n",
    "        similarity_matrix = cosine_similarity(tfidf, tfidf)\n",
    "\n",
    "        # TextRank 알고리즘을 적용하여 문서 요약\n",
    "        similarity_graph = similarity_matrix\n",
    "        nx_graph = nx.from_numpy_array(similarity_graph)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "\n",
    "        # 핵심 문장 추출\n",
    "        ranked_sentences = sorted(((score, index) for index, score in scores.items()), reverse=True)\n",
    "        selected_sentences = [sentences[index] for _, index in ranked_sentences[:num_sentences]]\n",
    "        return ' '.join(selected_sentences)\n",
    "\n",
    "    # 문서 요약\n",
    "    summary = summarize(document, num_sentences=3)\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACE WITH TEARS OF JOY\n"
     ]
    }
   ],
   "source": [
    "# 이모티콘\n",
    "import unicodedata\n",
    "print(unicodedata.name('😂'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newprj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37dcb171acb47c3d04f804a42efe3935da75a96b4220c54673b89fccb59f3177"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
